nohup: ignoring input
Running on hadoop, using /home/zheyi/sarahadoop/hadoop-0.20/bin/hadoop and HADOOP_CONF_DIR=
MAHOUT-JOB: /home/zheyi/mahout-distribution-0.7/mahout-examples-0.7-job.jar
13/05/22 11:06:15 INFO common.AbstractJob: Command line arguments: {--encoding=[UTF-8], --endPhase=[2147483647], --input=[/user/s117449/synthetic/d_100m_20_1k_10k_4.data], --maxHeapSize=[50], --method=[mapreduce], --minSupport=[5000000], --numGroups=[1000], --numTreeCacheEntries=[5], --output=[output/fpg-synthetic-support5000000], --splitterPattern=[[\ ]], --startPhase=[0], --tempDir=[temp]}
13/05/22 11:06:16 INFO common.HadoopUtil: Deleting output/fpg-synthetic-support5000000
13/05/22 11:06:16 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
13/05/22 11:06:22 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 183454 for s117449
13/05/22 11:06:22 INFO security.TokenCache: Got dt for hdfs://p-head03.alley.sara.nl:8020/tmp/hadoop-mapred/mapred/staging/s117449/.staging/job_201305151711_3947;uri=145.100.41.4:8020;t.service=145.100.41.4:8020
13/05/22 11:06:22 INFO input.FileInputFormat: Total input paths to process : 1
13/05/22 11:06:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
13/05/22 11:06:22 WARN snappy.LoadSnappy: Snappy native library not loaded
13/05/22 11:06:23 INFO mapred.JobClient: Running job: job_201305151711_3947
13/05/22 11:06:24 INFO mapred.JobClient:  map 0% reduce 0%
13/05/22 11:07:01 INFO mapred.JobClient:  map 1% reduce 0%
13/05/22 11:07:12 INFO mapred.JobClient:  map 2% reduce 0%
13/05/22 11:07:16 INFO mapred.JobClient:  map 3% reduce 0%
13/05/22 11:07:21 INFO mapred.JobClient:  map 4% reduce 0%
13/05/22 11:07:24 INFO mapred.JobClient:  map 5% reduce 0%
13/05/22 11:07:27 INFO mapred.JobClient:  map 6% reduce 0%
13/05/22 11:07:31 INFO mapred.JobClient:  map 7% reduce 0%
13/05/22 11:07:33 INFO mapred.JobClient:  map 8% reduce 0%
13/05/22 11:07:36 INFO mapred.JobClient:  map 9% reduce 0%
13/05/22 11:07:37 INFO mapred.JobClient:  map 10% reduce 0%
13/05/22 11:07:40 INFO mapred.JobClient:  map 11% reduce 0%
13/05/22 11:07:41 INFO mapred.JobClient:  map 12% reduce 0%
13/05/22 11:07:43 INFO mapred.JobClient:  map 13% reduce 0%
13/05/22 11:07:45 INFO mapred.JobClient:  map 14% reduce 0%
13/05/22 11:07:47 INFO mapred.JobClient:  map 15% reduce 0%
13/05/22 11:07:49 INFO mapred.JobClient:  map 16% reduce 0%
13/05/22 11:07:51 INFO mapred.JobClient:  map 17% reduce 0%
13/05/22 11:07:53 INFO mapred.JobClient:  map 18% reduce 0%
13/05/22 11:07:55 INFO mapred.JobClient:  map 19% reduce 0%
13/05/22 11:07:56 INFO mapred.JobClient:  map 20% reduce 0%
13/05/22 11:07:58 INFO mapred.JobClient:  map 21% reduce 0%
13/05/22 11:08:00 INFO mapred.JobClient:  map 22% reduce 0%
13/05/22 11:08:02 INFO mapred.JobClient:  map 23% reduce 0%
13/05/22 11:08:03 INFO mapred.JobClient:  map 24% reduce 0%
13/05/22 11:08:05 INFO mapred.JobClient:  map 25% reduce 0%
13/05/22 11:08:06 INFO mapred.JobClient:  map 26% reduce 0%
13/05/22 11:08:08 INFO mapred.JobClient:  map 27% reduce 0%
13/05/22 11:08:09 INFO mapred.JobClient:  map 28% reduce 0%
13/05/22 11:08:11 INFO mapred.JobClient:  map 29% reduce 0%
13/05/22 11:08:12 INFO mapred.JobClient:  map 30% reduce 0%
13/05/22 11:08:14 INFO mapred.JobClient:  map 31% reduce 0%
13/05/22 11:08:15 INFO mapred.JobClient:  map 32% reduce 0%
13/05/22 11:08:18 INFO mapred.JobClient:  map 33% reduce 0%
13/05/22 11:08:19 INFO mapred.JobClient:  map 34% reduce 0%
13/05/22 11:08:20 INFO mapred.JobClient:  map 35% reduce 0%
13/05/22 11:08:22 INFO mapred.JobClient:  map 36% reduce 0%
13/05/22 11:08:23 INFO mapred.JobClient:  map 37% reduce 0%
13/05/22 11:08:24 INFO mapred.JobClient:  map 38% reduce 0%
13/05/22 11:08:26 INFO mapred.JobClient:  map 39% reduce 0%
13/05/22 11:08:27 INFO mapred.JobClient:  map 40% reduce 0%
13/05/22 11:08:29 INFO mapred.JobClient:  map 41% reduce 0%
13/05/22 11:08:30 INFO mapred.JobClient:  map 42% reduce 0%
13/05/22 11:08:31 INFO mapred.JobClient:  map 43% reduce 0%
13/05/22 11:08:33 INFO mapred.JobClient:  map 44% reduce 0%
13/05/22 11:08:34 INFO mapred.JobClient:  map 45% reduce 0%
13/05/22 11:08:35 INFO mapred.JobClient:  map 46% reduce 0%
13/05/22 11:08:37 INFO mapred.JobClient:  map 47% reduce 0%
13/05/22 11:08:38 INFO mapred.JobClient:  map 48% reduce 0%
13/05/22 11:08:39 INFO mapred.JobClient:  map 49% reduce 0%
13/05/22 11:08:40 INFO mapred.JobClient:  map 50% reduce 0%
13/05/22 11:08:42 INFO mapred.JobClient:  map 51% reduce 0%
13/05/22 11:08:43 INFO mapred.JobClient:  map 52% reduce 0%
13/05/22 11:08:44 INFO mapred.JobClient:  map 53% reduce 0%
13/05/22 11:08:46 INFO mapred.JobClient:  map 54% reduce 0%
13/05/22 11:08:47 INFO mapred.JobClient:  map 55% reduce 0%
13/05/22 11:08:48 INFO mapred.JobClient:  map 56% reduce 0%
13/05/22 11:08:49 INFO mapred.JobClient:  map 57% reduce 0%
13/05/22 11:08:51 INFO mapred.JobClient:  map 58% reduce 0%
13/05/22 11:08:52 INFO mapred.JobClient:  map 59% reduce 0%
13/05/22 11:08:53 INFO mapred.JobClient:  map 60% reduce 0%
13/05/22 11:08:54 INFO mapred.JobClient:  map 61% reduce 0%
13/05/22 11:08:56 INFO mapred.JobClient:  map 62% reduce 0%
13/05/22 11:08:57 INFO mapred.JobClient:  map 63% reduce 0%
13/05/22 11:08:58 INFO mapred.JobClient:  map 64% reduce 0%
13/05/22 11:08:59 INFO mapred.JobClient:  map 65% reduce 0%
13/05/22 11:09:00 INFO mapred.JobClient:  map 66% reduce 0%
13/05/22 11:09:01 INFO mapred.JobClient:  map 67% reduce 0%
13/05/22 11:09:03 INFO mapred.JobClient:  map 68% reduce 0%
13/05/22 11:09:04 INFO mapred.JobClient:  map 69% reduce 0%
13/05/22 11:09:05 INFO mapred.JobClient:  map 70% reduce 0%
13/05/22 11:09:07 INFO mapred.JobClient:  map 71% reduce 0%
13/05/22 11:09:08 INFO mapred.JobClient:  map 72% reduce 0%
13/05/22 11:09:09 INFO mapred.JobClient:  map 73% reduce 0%
13/05/22 11:09:10 INFO mapred.JobClient:  map 74% reduce 0%
13/05/22 11:09:12 INFO mapred.JobClient:  map 75% reduce 0%
13/05/22 11:09:14 INFO mapred.JobClient:  map 76% reduce 0%
13/05/22 11:09:15 INFO mapred.JobClient:  map 77% reduce 0%
13/05/22 11:09:17 INFO mapred.JobClient:  map 78% reduce 0%
13/05/22 11:09:18 INFO mapred.JobClient:  map 79% reduce 0%
13/05/22 11:09:19 INFO mapred.JobClient:  map 80% reduce 0%
13/05/22 11:09:20 INFO mapred.JobClient:  map 81% reduce 0%
13/05/22 11:09:22 INFO mapred.JobClient:  map 82% reduce 0%
13/05/22 11:09:23 INFO mapred.JobClient:  map 83% reduce 0%
13/05/22 11:09:25 INFO mapred.JobClient:  map 84% reduce 0%
13/05/22 11:09:26 INFO mapred.JobClient:  map 85% reduce 0%
13/05/22 11:09:27 INFO mapred.JobClient:  map 86% reduce 0%
13/05/22 11:09:29 INFO mapred.JobClient:  map 87% reduce 0%
13/05/22 11:09:31 INFO mapred.JobClient:  map 88% reduce 0%
13/05/22 11:09:33 INFO mapred.JobClient:  map 89% reduce 0%
13/05/22 11:09:34 INFO mapred.JobClient:  map 90% reduce 0%
13/05/22 11:09:36 INFO mapred.JobClient:  map 91% reduce 0%
13/05/22 11:09:38 INFO mapred.JobClient:  map 92% reduce 0%
13/05/22 11:09:40 INFO mapred.JobClient:  map 93% reduce 0%
13/05/22 11:09:43 INFO mapred.JobClient:  map 94% reduce 3%
13/05/22 11:09:45 INFO mapred.JobClient:  map 95% reduce 3%
13/05/22 11:09:46 INFO mapred.JobClient:  map 95% reduce 5%
13/05/22 11:09:47 INFO mapred.JobClient:  map 96% reduce 5%
13/05/22 11:09:49 INFO mapred.JobClient:  map 96% reduce 6%
13/05/22 11:09:50 INFO mapred.JobClient:  map 97% reduce 6%
13/05/22 11:09:53 INFO mapred.JobClient:  map 98% reduce 6%
13/05/22 11:09:57 INFO mapred.JobClient:  map 99% reduce 6%
13/05/22 11:09:59 INFO mapred.JobClient:  map 99% reduce 12%
13/05/22 11:10:39 INFO mapred.JobClient:  map 100% reduce 12%
13/05/22 11:12:40 INFO mapred.JobClient:  map 100% reduce 20%
13/05/22 11:16:44 INFO mapred.JobClient:  map 100% reduce 25%
13/05/22 11:16:47 INFO mapred.JobClient:  map 100% reduce 32%
13/05/22 11:19:31 INFO mapred.JobClient:  map 100% reduce 33%
13/05/22 11:20:31 INFO mapred.JobClient:  map 100% reduce 66%
13/05/22 11:21:00 INFO mapred.JobClient:  map 100% reduce 67%
13/05/22 11:22:01 INFO mapred.JobClient:  map 100% reduce 68%
13/05/22 11:22:52 INFO mapred.JobClient:  map 100% reduce 69%
13/05/22 11:23:42 INFO mapred.JobClient:  map 100% reduce 70%
13/05/22 11:24:34 INFO mapred.JobClient:  map 100% reduce 71%
13/05/22 11:24:38 INFO mapred.JobClient:  map 100% reduce 100%
13/05/22 11:24:39 INFO mapred.JobClient: Job complete: job_201305151711_3947
13/05/22 11:24:39 INFO mapred.JobClient: Counters: 27
13/05/22 11:24:39 INFO mapred.JobClient:   Job Counters 
13/05/22 11:24:39 INFO mapred.JobClient:     Launched reduce tasks=1
13/05/22 11:24:39 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=14119593
13/05/22 11:24:39 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
13/05/22 11:24:39 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
13/05/22 11:24:39 INFO mapred.JobClient:     Rack-local map tasks=75
13/05/22 11:24:39 INFO mapred.JobClient:     Launched map tasks=85
13/05/22 11:24:39 INFO mapred.JobClient:     Data-local map tasks=8
13/05/22 11:24:39 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=902705
13/05/22 11:24:39 INFO mapred.JobClient:   FileSystemCounters
13/05/22 11:24:39 INFO mapred.JobClient:     FILE_BYTES_READ=861661621
13/05/22 11:24:39 INFO mapred.JobClient:     HDFS_BYTES_READ=9825694887
13/05/22 11:24:39 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=1116378162
13/05/22 11:24:39 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=2513580322
13/05/22 11:24:39 INFO mapred.JobClient:   Map-Reduce Framework
13/05/22 11:24:39 INFO mapred.JobClient:     Map input records=100000000
13/05/22 11:24:39 INFO mapred.JobClient:     Reduce shuffle bytes=250125758
13/05/22 11:24:39 INFO mapred.JobClient:     Spilled Records=437285308
13/05/22 11:24:39 INFO mapred.JobClient:     Map output bytes=28169645452
13/05/22 11:24:39 INFO mapred.JobClient:     CPU time spent (ms)=13229550
13/05/22 11:24:39 INFO mapred.JobClient:     Total committed heap usage (bytes)=73468084224
13/05/22 11:24:39 INFO mapred.JobClient:     Combine input records=2489264673
13/05/22 11:24:39 INFO mapred.JobClient:     SPLIT_RAW_BYTES=10434
13/05/22 11:24:39 INFO mapred.JobClient:     Reduce input records=100010638
13/05/22 11:24:39 INFO mapred.JobClient:     Reduce input groups=100000001
13/05/22 11:24:39 INFO mapred.JobClient:     Combine output records=296242801
13/05/22 11:24:39 INFO mapred.JobClient:     Physical memory (bytes) snapshot=39852425216
13/05/22 11:24:39 INFO mapred.JobClient:     Reduce output records=100000001
13/05/22 11:24:39 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=120725733376
13/05/22 11:24:39 INFO mapred.JobClient:     Map output records=2293032510
13/05/22 11:31:04 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.
13/05/22 11:31:04 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 183469 for s117449
13/05/22 11:31:04 INFO security.TokenCache: Got dt for hdfs://p-head03.alley.sara.nl/user/s117449/output/fpg-synthetic-support5000000/fList;uri=145.100.41.4:8020;t.service=145.100.41.4:8020
13/05/22 11:31:08 INFO input.FileInputFormat: Total input paths to process : 1
13/05/22 11:31:09 INFO mapred.JobClient: Running job: job_201305151711_3962
13/05/22 11:31:10 INFO mapred.JobClient:  map 0% reduce 0%
13/05/22 11:31:48 INFO mapred.JobClient:  map 1% reduce 0%
13/05/22 11:31:57 INFO mapred.JobClient:  map 2% reduce 0%
13/05/22 11:32:01 INFO mapred.JobClient:  map 3% reduce 0%
13/05/22 11:32:06 INFO mapred.JobClient:  map 4% reduce 0%
13/05/22 11:32:11 INFO mapred.JobClient:  map 5% reduce 0%
13/05/22 11:32:14 INFO mapred.JobClient:  map 6% reduce 0%
13/05/22 11:32:16 INFO mapred.JobClient:  map 7% reduce 0%
13/05/22 11:32:18 INFO mapred.JobClient:  map 8% reduce 0%
13/05/22 11:32:21 INFO mapred.JobClient:  map 9% reduce 0%
13/05/22 11:32:24 INFO mapred.JobClient:  map 10% reduce 0%
13/05/22 11:32:25 INFO mapred.JobClient:  map 11% reduce 0%
13/05/22 11:32:27 INFO mapred.JobClient:  map 12% reduce 0%
13/05/22 11:32:29 INFO mapred.JobClient:  map 13% reduce 0%
13/05/22 11:32:30 INFO mapred.JobClient:  map 14% reduce 0%
13/05/22 11:32:32 INFO mapred.JobClient:  map 15% reduce 0%
13/05/22 11:32:33 INFO mapred.JobClient:  map 16% reduce 0%
13/05/22 11:32:34 INFO mapred.JobClient:  map 17% reduce 0%
13/05/22 11:32:36 INFO mapred.JobClient:  map 18% reduce 0%
13/05/22 11:32:37 INFO mapred.JobClient:  map 19% reduce 0%
13/05/22 11:32:38 INFO mapred.JobClient:  map 20% reduce 0%
13/05/22 11:32:39 INFO mapred.JobClient:  map 21% reduce 0%
13/05/22 11:32:40 INFO mapred.JobClient:  map 22% reduce 0%
13/05/22 11:32:41 INFO mapred.JobClient:  map 23% reduce 0%
13/05/22 11:32:42 INFO mapred.JobClient:  map 24% reduce 0%
13/05/22 11:32:43 INFO mapred.JobClient:  map 25% reduce 0%
13/05/22 11:32:44 INFO mapred.JobClient:  map 26% reduce 0%
13/05/22 11:32:45 INFO mapred.JobClient:  map 27% reduce 0%
13/05/22 11:32:46 INFO mapred.JobClient:  map 28% reduce 0%
13/05/22 11:32:47 INFO mapred.JobClient:  map 29% reduce 0%
13/05/22 11:32:48 INFO mapred.JobClient:  map 30% reduce 0%
13/05/22 11:32:49 INFO mapred.JobClient:  map 31% reduce 0%
13/05/22 11:32:50 INFO mapred.JobClient:  map 32% reduce 0%
13/05/22 11:32:51 INFO mapred.JobClient:  map 33% reduce 0%
13/05/22 11:32:52 INFO mapred.JobClient:  map 35% reduce 0%
13/05/22 11:32:54 INFO mapred.JobClient:  map 37% reduce 0%
13/05/22 11:32:55 INFO mapred.JobClient:  map 38% reduce 0%
13/05/22 11:32:56 INFO mapred.JobClient:  map 39% reduce 0%
13/05/22 11:32:57 INFO mapred.JobClient:  map 40% reduce 0%
13/05/22 11:32:58 INFO mapred.JobClient:  map 41% reduce 0%
13/05/22 11:32:59 INFO mapred.JobClient:  map 42% reduce 0%
13/05/22 11:33:00 INFO mapred.JobClient:  map 43% reduce 0%
13/05/22 11:33:01 INFO mapred.JobClient:  map 44% reduce 0%
13/05/22 11:33:02 INFO mapred.JobClient:  map 45% reduce 0%
13/05/22 11:33:03 INFO mapred.JobClient:  map 47% reduce 0%
13/05/22 11:33:04 INFO mapred.JobClient:  map 48% reduce 0%
13/05/22 11:33:05 INFO mapred.JobClient:  map 49% reduce 0%
13/05/22 11:33:06 INFO mapred.JobClient:  map 51% reduce 0%
13/05/22 11:33:08 INFO mapred.JobClient:  map 53% reduce 0%
13/05/22 11:33:09 INFO mapred.JobClient:  map 54% reduce 0%
13/05/22 11:33:10 INFO mapred.JobClient:  map 55% reduce 0%
13/05/22 11:33:11 INFO mapred.JobClient:  map 56% reduce 0%
13/05/22 11:33:12 INFO mapred.JobClient:  map 57% reduce 0%
13/05/22 11:33:13 INFO mapred.JobClient:  map 59% reduce 0%
13/05/22 11:33:14 INFO mapred.JobClient:  map 60% reduce 0%
13/05/22 11:33:15 INFO mapred.JobClient:  map 61% reduce 0%
13/05/22 11:33:16 INFO mapred.JobClient:  map 62% reduce 0%
13/05/22 11:33:17 INFO mapred.JobClient:  map 63% reduce 0%
13/05/22 11:33:18 INFO mapred.JobClient:  map 64% reduce 0%
13/05/22 11:33:19 INFO mapred.JobClient:  map 66% reduce 0%
13/05/22 11:33:20 INFO mapred.JobClient:  map 67% reduce 0%
13/05/22 11:33:21 INFO mapred.JobClient:  map 68% reduce 0%
13/05/22 11:33:22 INFO mapred.JobClient:  map 69% reduce 0%
13/05/22 11:33:23 INFO mapred.JobClient:  map 70% reduce 0%
13/05/22 11:33:24 INFO mapred.JobClient:  map 72% reduce 0%
13/05/22 11:33:25 INFO mapred.JobClient:  map 73% reduce 0%
13/05/22 11:33:26 INFO mapred.JobClient:  map 74% reduce 0%
13/05/22 11:33:27 INFO mapred.JobClient:  map 76% reduce 0%
13/05/22 11:33:28 INFO mapred.JobClient:  map 77% reduce 0%
13/05/22 11:33:29 INFO mapred.JobClient:  map 78% reduce 0%
13/05/22 11:33:30 INFO mapred.JobClient:  map 79% reduce 0%
13/05/22 11:33:32 INFO mapred.JobClient:  map 80% reduce 0%
13/05/22 11:33:33 INFO mapred.JobClient:  map 81% reduce 0%
13/05/22 11:33:34 INFO mapred.JobClient:  map 82% reduce 0%
13/05/22 11:33:35 INFO mapred.JobClient:  map 83% reduce 0%
13/05/22 11:33:36 INFO mapred.JobClient:  map 84% reduce 0%
13/05/22 11:33:37 INFO mapred.JobClient:  map 86% reduce 0%
13/05/22 11:33:38 INFO mapred.JobClient:  map 87% reduce 0%
13/05/22 11:33:40 INFO mapred.JobClient:  map 88% reduce 0%
13/05/22 11:33:41 INFO mapred.JobClient:  map 90% reduce 0%
13/05/22 11:33:43 INFO mapred.JobClient:  map 91% reduce 0%
13/05/22 11:33:44 INFO mapred.JobClient:  map 92% reduce 0%
13/05/22 11:33:45 INFO mapred.JobClient:  map 93% reduce 0%
13/05/22 11:33:47 INFO mapred.JobClient:  map 94% reduce 0%
13/05/22 11:33:49 INFO mapred.JobClient:  map 95% reduce 0%
13/05/22 11:33:50 INFO mapred.JobClient:  map 95% reduce 1%
13/05/22 11:33:51 INFO mapred.JobClient:  map 96% reduce 1%
13/05/22 11:33:53 INFO mapred.JobClient:  map 96% reduce 2%
13/05/22 11:33:54 INFO mapred.JobClient:  map 97% reduce 2%
13/05/22 11:33:58 INFO mapred.JobClient:  map 98% reduce 2%
13/05/22 11:34:04 INFO mapred.JobClient:  map 99% reduce 2%
13/05/22 11:34:13 INFO mapred.JobClient:  map 99% reduce 3%
13/05/22 11:34:16 INFO mapred.JobClient:  map 99% reduce 4%
13/05/22 11:34:30 INFO mapred.JobClient:  map 99% reduce 5%
13/05/22 11:34:33 INFO mapred.JobClient:  map 99% reduce 7%
13/05/22 11:34:36 INFO mapred.JobClient:  map 99% reduce 9%
13/05/22 11:34:45 INFO mapred.JobClient:  map 100% reduce 9%
13/05/22 11:37:07 INFO mapred.JobClient:  map 100% reduce 0%
13/05/22 11:37:07 INFO mapred.JobClient: Task Id : attempt_201305151711_3962_r_000000_0, Status : FAILED
Error: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.resizeChildren(TransactionTree.java:382)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.addChild(TransactionTree.java:98)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.createNode(TransactionTree.java:339)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.addPattern(TransactionTree.java:130)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.getCompressedTree(TransactionTree.java:201)
	at org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthCombiner.reduce(ParallelFPGrowthCombiner.java:43)
	at org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthCombiner.reduce(ParallelFPGrowthCombiner.java:32)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1466)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2735)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFS
attempt_201305151711_3962_r_000000_0: Exception in thread "Timer thread for monitoring jvm" java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_0: 	at java.nio.ByteBuffer.wrap(ByteBuffer.java:350)
attempt_201305151711_3962_r_000000_0: 	at java.nio.ByteBuffer.wrap(ByteBuffer.java:373)
attempt_201305151711_3962_r_000000_0: 	at java.lang.StringCoding$StringEncoder.encode(StringCoding.java:237)
attempt_201305151711_3962_r_000000_0: 	at java.lang.StringCoding.encode(StringCoding.java:272)
attempt_201305151711_3962_r_000000_0: 	at java.lang.StringCoding.encode(StringCoding.java:284)
attempt_201305151711_3962_r_000000_0: 	at java.lang.String.getBytes(String.java:986)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.ganglia.GangliaContext.xdr_string(GangliaContext.java:213)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.ganglia.GangliaContext31.emitMetric(GangliaContext31.java:129)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.ganglia.GangliaContext.emitRecord(GangliaContext.java:133)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.spi.AbstractMetricsContext.emitRecords(AbstractMetricsContext.java:313)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.spi.AbstractMetricsContext.timerEvent(AbstractMetricsContext.java:299)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.spi.AbstractMetricsContext.access$000(AbstractMetricsContext.java:53)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.metrics.spi.AbstractMetricsContext$1.run(AbstractMetricsContext.java:258)
attempt_201305151711_3962_r_000000_0: 	at java.util.TimerThread.mainLoop(Timer.java:512)
attempt_201305151711_3962_r_000000_0: 	at java.util.TimerThread.run(Timer.java:462)
attempt_201305151711_3962_r_000000_0: Exception in thread "Thread for syncLogs" java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_0: 	at java.lang.StringCoding.encode(StringCoding.java:266)
attempt_201305151711_3962_r_000000_0: 	at java.lang.String.getBytes(String.java:946)
attempt_201305151711_3962_r_000000_0: 	at java.io.UnixFileSystem.rename0(Native Method)
attempt_201305151711_3962_r_000000_0: 	at java.io.UnixFileSystem.rename(UnixFileSystem.java:264)
attempt_201305151711_3962_r_000000_0: 	at java.io.File.renameTo(File.java:1228)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:284)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:429)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:351)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:397)
attempt_201305151711_3962_r_000000_0: 	at org.apache.hadoop.mapred.Child$3.run(Child.java:153)
13/05/22 11:37:35 INFO mapred.JobClient:  map 100% reduce 6%
13/05/22 11:37:41 INFO mapred.JobClient:  map 100% reduce 9%
13/05/22 11:39:34 INFO mapred.JobClient:  map 100% reduce 0%
13/05/22 11:39:36 INFO mapred.JobClient: Task Id : attempt_201305151711_3962_r_000000_1, Status : FAILED
Error: java.lang.OutOfMemoryError: Java heap space
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.resize(TransactionTree.java:367)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.createNode(TransactionTree.java:328)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.addPattern(TransactionTree.java:130)
	at org.apache.mahout.fpm.pfpgrowth.TransactionTree.getCompressedTree(TransactionTree.java:201)
	at org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthCombiner.reduce(ParallelFPGrowthCombiner.java:43)
	at org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthCombiner.reduce(ParallelFPGrowthCombiner.java:32)
	at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)
	at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1466)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2735)
	at org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2673)

13/05/22 11:40:04 INFO mapred.JobClient:  map 100% reduce 9%
13/05/22 11:42:16 INFO mapred.JobClient:  map 100% reduce 0%
13/05/22 11:42:17 INFO mapred.JobClient: Task Id : attempt_201305151711_3962_r_000000_2, Status : FAILED
Error: java.lang.OutOfMemoryError: GC overhead limit exceeded

attempt_201305151711_3962_r_000000_2: Exception in thread "Timer thread for monitoring mapred" java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_2: java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_2: Exception in thread "Timer thread for monitoring jvm" java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_2: Exception in thread "main" java.lang.OutOfMemoryError: GC overhead limit exceeded
attempt_201305151711_3962_r_000000_2: log4j:WARN No appenders could be found for logger (org.apache.hadoop.mapred.Task).
attempt_201305151711_3962_r_000000_2: log4j:WARN Please initialize the log4j system properly.
13/05/22 11:42:36 INFO mapred.JobClient:  map 100% reduce 9%
13/05/22 11:43:23 INFO mapred.JobClient:  map 100% reduce 0%
13/05/22 11:43:25 INFO mapred.JobClient: Job complete: job_201305151711_3962
13/05/22 11:43:25 INFO mapred.JobClient: Counters: 23
13/05/22 11:43:25 INFO mapred.JobClient:   Job Counters 
13/05/22 11:43:25 INFO mapred.JobClient:     Launched reduce tasks=4
13/05/22 11:43:25 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=12548557
13/05/22 11:43:25 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
13/05/22 11:43:25 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
13/05/22 11:43:25 INFO mapred.JobClient:     Rack-local map tasks=67
13/05/22 11:43:25 INFO mapred.JobClient:     Launched map tasks=89
13/05/22 11:43:25 INFO mapred.JobClient:     Data-local map tasks=17
13/05/22 11:43:25 INFO mapred.JobClient:     Failed reduce tasks=1
13/05/22 11:43:25 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=6326
13/05/22 11:43:25 INFO mapred.JobClient:   FileSystemCounters
13/05/22 11:43:25 INFO mapred.JobClient:     FILE_BYTES_READ=3155744235
13/05/22 11:43:25 INFO mapred.JobClient:     HDFS_BYTES_READ=9825694887
13/05/22 11:43:25 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=4436207088
13/05/22 11:43:25 INFO mapred.JobClient:   Map-Reduce Framework
13/05/22 11:43:25 INFO mapred.JobClient:     Combine output records=257545
13/05/22 11:43:25 INFO mapred.JobClient:     Map input records=100000000
13/05/22 11:43:25 INFO mapred.JobClient:     Physical memory (bytes) snapshot=43682557952
13/05/22 11:43:25 INFO mapred.JobClient:     Spilled Records=458755
13/05/22 11:43:25 INFO mapred.JobClient:     Map output bytes=8349106065
13/05/22 11:43:25 INFO mapred.JobClient:     CPU time spent (ms)=10681720
13/05/22 11:43:25 INFO mapred.JobClient:     Total committed heap usage (bytes)=75318951936
13/05/22 11:43:25 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=119031451648
13/05/22 11:43:25 INFO mapred.JobClient:     Combine input records=677110333
13/05/22 11:43:25 INFO mapred.JobClient:     Map output records=676859818
13/05/22 11:43:25 INFO mapred.JobClient:     SPLIT_RAW_BYTES=10434
Exception in thread "main" java.lang.IllegalStateException: Job failed!
	at org.apache.mahout.fpm.pfpgrowth.PFPGrowth.startParallelFPGrowth(PFPGrowth.java:352)
	at org.apache.mahout.fpm.pfpgrowth.PFPGrowth.runPFPGrowth(PFPGrowth.java:245)
	at org.apache.mahout.fpm.pfpgrowth.FPGrowthDriver.run(FPGrowthDriver.java:136)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.mahout.fpm.pfpgrowth.FPGrowthDriver.main(FPGrowthDriver.java:56)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)
	at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:197)
